{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrgQVPDOZvbN",
        "outputId": "90011b0a-da85-4fec-c5e4-3bc7de3f5d07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import nltk\n",
        "nltk.download()\n",
        "\n",
        "import jieba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/seanzhang-zhichen/-transformer-english2chinese-.git"
      ],
      "metadata": {
        "id": "y7dC-RDNffSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9ad47f-201a-49a2-fa44-3eaa94b1eb9e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '-transformer-english2chinese-' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(in_file):\n",
        "    cn = []\n",
        "    en = []\n",
        "    num_examples = 0\n",
        "    with open(in_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split('\\t')      # 每一行是英文+翻译的形式\n",
        "            #print(line)   # ['Anyone can do that.', '任何人都可以做到。']\n",
        "            #print(nltk.word_tokenize(line[0].lower()))    # ['anyone', 'can', 'do', 'that', '.']\n",
        "            en.append(['BOS'] + nltk.word_tokenize(line[0].lower()) + ['EOS'])\n",
        "            #print([c for c in line[1]])   ['任', '何', '人', '都', '可', '以', '做', '到', '。']\n",
        "            #print(list(jieba.cut(line[1])))        ['任何人', '都', '可以', '做到', '。']\n",
        "            #cn.append(['BOS'] + [c for c in line[1]] + ['EOS'])\n",
        "            cn.append(['BOS'] + list(jieba.cut(line[1])) + ['EOS'])\n",
        "    return en, cn\n",
        "\n",
        "train_file = '/content/-transformer-english2chinese-/nmt/en-cn/train.txt'\n",
        "dev_file = '/content/-transformer-english2chinese-/nmt/en-cn/dev.txt'\n",
        "train_en, train_cn = load_data(train_file)\n",
        "dev_en, dev_cn = load_data(dev_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4qq88VXaXGs",
        "outputId": "b52e16da-e652-4c25-dbe1-1f523e5e4f5e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.726 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.726 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_en[:3],train_cn[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4UjZr-OgoyJ",
        "outputId": "52e27579-0225-45a3-b381-00f69c58cab0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'], ['BOS', 'how', 'about', 'another', 'piece', 'of', 'cake', '?', 'EOS'], ['BOS', 'she', 'married', 'him', '.', 'EOS']] [['BOS', '任何人', '都', '可以', '做到', '。', 'EOS'], ['BOS', '要', '不要', '再來', '一塊', '蛋糕', '？', 'EOS'], ['BOS', '她', '嫁给', '了', '他', '。', 'EOS']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "UNK_IDX = 0\n",
        "PAD_IDX = 1\n",
        "def build_dict(sentences, max_words=50000):\n",
        "    word_count = Counter()\n",
        "    for sentence in sentences:\n",
        "        for s in sentence:\n",
        "            word_count[s] += 1\n",
        "    ls = word_count.most_common(max_words)\n",
        "    total_words = len(ls) + 2    # 两个特殊的字符UNK和PAD\n",
        "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}   # 字典的前两个位置放特殊字符\n",
        "    word_dict['UNK'] = UNK_IDX\n",
        "    word_dict['PAD'] = PAD_IDX\n",
        "    return word_dict, total_words\n",
        "\n",
        "en_dict, en_total_words = build_dict(train_en)\n",
        "cn_dict, cn_total_words = build_dict(train_cn)\n",
        "\n",
        "inv_en_dict = {v:k for k, v in en_dict.items()}\n",
        "inv_cn_dict = {v:k for k, v in cn_dict.items()}"
      ],
      "metadata": {
        "id": "aI7TcoO7g7qN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
        "    length = len(en_sentences)\n",
        "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
        "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
        "    # 根据英语句子的长度排序\n",
        "    def len_argsort(seq):   # 这个seq是一个二维矩阵， 每一行是一个句子， 且都已经用单词在字典中的位置进行了编码\n",
        "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
        "\n",
        "    if sort_by_len:\n",
        "        sorted_index = len_argsort(out_en_sentences)\n",
        "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
        "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
        "\n",
        "    return out_en_sentences, out_cn_sentences\n",
        "\n",
        "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
        "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
      ],
      "metadata": {
        "id": "M4GCzU2XhA4w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 499\n",
        "print(\" \".join([inv_cn_dict[i] for i in train_cn[k]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZXGpfO6hDfs",
        "outputId": "02ab2d20-3f97-4b04-c6e8-939e255d9277"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOS 我们 该 步行 吗 ？ EOS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" \".join([inv_en_dict[i] for i in train_en[k]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muGxe3wyh8oK",
        "outputId": "4ace97d7-45c8-497e-b946-925a663ca033"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOS shall we walk ? EOS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 这个函数的作用是我们输入训练集的样本个数， batch_size大小， 就会返回多批 连续的batch_size个索引， 每一个索引代表一个样本\n",
        "# 也就是可以根据这个索引去拿到一个个的batch\n",
        "def get_minibatches(n, minibatch_size, shuffle=True):\n",
        "    idx_list = np.arange(0, n, minibatch_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx_list)\n",
        "    minibatches = []\n",
        "    for idx in idx_list:\n",
        "        minibatches.append(np.arange(idx, min(idx+minibatch_size, n)))\n",
        "    return minibatches      # 这个会返回多批连着的bath_size个索引\n",
        "#get_minibatches(len(train_en), 32)\n",
        "\n",
        "# 这个函数是在做数据预处理， 由于每个句子都不是一样长， 所以通过这个函数就可以把句子进行补齐， 不够长的在句子后面添加0\n",
        "def prepare_data(seqs):\n",
        "    lengths = [len(seq) for seq in seqs]    # 得到每个句子的长度\n",
        "    n_samples = len(seqs)       # 得到一共有多少个句子\n",
        "    max_len = np.max(lengths)              # 找出最大的句子长度\n",
        "\n",
        "    x = np.zeros((n_samples, max_len)).astype('int32')    # 按照最大句子长度生成全0矩阵\n",
        "    x_lengths = np.array(lengths).astype('int32')\n",
        "    for idx, seq in enumerate(seqs):        # 把有句子的位置填充进去\n",
        "        x[idx, :lengths[idx]] = seq\n",
        "    return x, x_lengths      # x_mask\n",
        "\n",
        "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
        "    minibatches = get_minibatches(len(en_sentences), batch_size)   # 得到batch个索引\n",
        "    all_ex = []\n",
        "    for minibatch in minibatches:   # 每批数据的索引\n",
        "        mb_en_sentences = [en_sentences[t] for t in minibatch]   # 取数据\n",
        "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]  # 取数据\n",
        "        mb_x, mb_x_len = prepare_data(mb_en_sentences) # 填充成一样的长度， 但是要记录一下句子的真实长度， 这个在后面输入网络的时候得用\n",
        "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
        "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
        "    return all_ex\n",
        "\n",
        "batch_size = 64\n",
        "train_data = gen_examples(train_en, train_cn, batch_size)   # 产生训练集\n",
        "random.shuffle(train_data)\n",
        "dev_data = gen_examples(dev_en, dev_cn, batch_size)   # 产生验证集"
      ],
      "metadata": {
        "id": "57s4qt8biKnE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[1][0].shape, train_data[1][1].shape, train_data[1][2].shape, train_data[1][3].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXUqDuuFiVSs",
        "outputId": "9b8551a5-b201-4851-ab65-5bfc3ee615be"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10) (64,) (64, 15) (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "tensor_in = torch.FloatTensor([[1, 2, 3], [5, 0, 0]]).resize_(2, 3, 1)\n",
        "tensor_in = Variable(tensor_in)\n",
        "seq_lenghs = [3, 1]\n",
        "tensor_in"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rImXQHMdiz5Z",
        "outputId": "c46ef7eb-1847-4a35-b7fa-d76573302f39"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.],\n",
              "         [2.],\n",
              "         [3.]],\n",
              "\n",
              "        [[5.],\n",
              "         [0.],\n",
              "         [0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(seq_lenghs).sort(0, descending=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HauS6qJZjNIl",
        "outputId": "5c7eb2c5-9cc2-493b-8891-3ea4a9fb1b95"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.sort(\n",
              "values=tensor([3, 1]),\n",
              "indices=tensor([0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pack = nn.utils.rnn.pack_padded_sequence(tensor_in, seq_lenghs, batch_first=True)\n",
        "pack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBvxHw-rkNAi",
        "outputId": "c87a4f05-7ea5-4330-cd2f-245f770c4ca5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[1.],\n",
              "        [5.],\n",
              "        [2.],\n",
              "        [3.]]), batch_sizes=tensor([2, 1, 1]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = nn.RNN(1, 2, 3, batch_first=True)   # 输入维度是1(embed_dim)， 输出维度是2(2个隐藏单元), 3层\n",
        "h0 = Variable(torch.randn(3, 2, 2))  # h0的初始状态， (layers_num*direction_nums, batch_size, hidden_size)\n",
        "\n",
        "out, h = rnn(pack, h0)\n",
        "out[0].shape   # [4, 2]\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id_93ehVlsqQ",
        "outputId": "836d5357-4a05-40eb-920d-b0dbb01378e4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[-0.3927, -0.0730],\n",
              "        [-0.9640,  0.1754],\n",
              "        [-0.8934, -0.0802],\n",
              "        [-0.9538, -0.0276]], grad_fn=<CatBackward0>), batch_sizes=tensor([2, 1, 1]), sorted_indices=None, unsorted_indices=None)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PlainEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
        "        super(PlainEncoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        #这里需要输入lengths, 因为每个句子是不一样长的，我们需要每个句子最后一个时间步的隐藏状态，\n",
        "        #所以需要知道句子有多长，x表示一个batch里面的句子\n",
        "\n",
        "        #把batch里面的seq按照长度排序\n",
        "        sorten, sorted_idx = lengths.sort(0, descending=True) #sorten表示排好序的数组，sorted_index表示每个元素再原数组位置\n",
        "        x_sorted = x[sorted_idx.long()] #句子已经按照seq长度排好序\n",
        "        embedded = self.dropout(self.embed(x_sorted)) #[batch_size, seq_len, embed_size]\n",
        "\n",
        "        #下面一段代码处理变长序列\n",
        "        #这里的data.numpy()是原始张量的克隆，然后转成了numpy数组，相当于clone().numpy()\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorten.long().cpu().data.numpy(), batch_first=True)\n",
        "        #上面这句话之后，会把变长序列的0都给去掉，之前填充的字符都给压扁\n",
        "        packed_out, hid = self.rnn(packed_embedded)#通过这句话就可以得到batch中每个样本的真实隐藏状态\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)#这里是再填充回去，看下面的例子就懂了\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False) #这里是为了还是让短的句子再前面\n",
        "        out = out[original_idx.long()].contiguous() #contiguous是为了把不连续的内存单元连续起来\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "\n",
        "        return out, hid[[-1]] #把最后一层的his给拿出来，这个具体看上面的简单演示"
      ],
      "metadata": {
        "id": "Xiz9IgxWmhqT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlainDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
        "        super(PlainDecoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, y, y_lengths, hid):\n",
        "        #y:[batch_size, seq_len-1]\n",
        "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True) #依然是句子从长到短排序\n",
        "        y_sorted = y[sorted_idx.long()]\n",
        "        hid = hid[:, sorted_idx.long()]\n",
        "\n",
        "        y_sorted = self.dropout(self.embed(y_sorted)) #[batch_size, output_length, embed_size]\n",
        "\n",
        "        pack_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        out, hid = self.rnn(pack_seq, hid) #这个计算得是每个有效时间步单词得最后一层得隐藏状态\n",
        "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True) #[batch,seq_len-1, hidden_size]\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        output_seq = unpacked[original_idx.long()].contiguous() #[batch, seq_len-1, hidden_size]\n",
        "\n",
        "        hid = hid[:, original_idx.long()].contiguous() #[1,batch, hidden_size]\n",
        "        output = F.log_softmax(self.out(output_seq), -1)\n",
        "        #[batch, seq_len-1, vocab_size] 表示每个样本每个时间步长都有一个vocab_size得维度长度，表示每个单词得概率\n",
        "\n",
        "        return output, hid"
      ],
      "metadata": {
        "id": "K4COn0dsmpY8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlainSeq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(PlainSeq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    def forward(self, x, x_lengths, y, y_lengths):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths) #encoder进行编码\n",
        "        output, hid = self.decoder(y, y_lengths, hid) #decoder负责解码\n",
        "        return output, None\n",
        "    def translate(self, x, x_lengths, y, max_length=10):#这个是进来一个句子进行翻译 max_length句子得最大长度\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        preds = []\n",
        "        batch_size = x.shape[0]\n",
        "        attns = []\n",
        "        for i in range(max_length):\n",
        "            output, hid = self.decoder(y, torch.ones(batch_size).long().to(y.device), hid=hid)\n",
        "            y = output.max(2)[1].view(batch_size, 1)\n",
        "            preds.append(y)\n",
        "        return torch.cat(preds, 1), None"
      ],
      "metadata": {
        "id": "gynimbXomtDE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# masked cross entropy loss\n",
        "class LanguageModelCriterion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LanguageModelCriterion, self).__init__()\n",
        "\n",
        "    def forward(self, input, target, mask):\n",
        "        # input: [batch_size, seq_len, vocab_size]    每个单词的可能性contiguous\n",
        "        input = input.contiguous().view(-1, input.size(2))   # [batch_size*seq_len-1, vocab_size]\n",
        "        target = target.contiguous().view(-1, 1)    #  [batch_size*seq_len-1, 1]\n",
        "\n",
        "        mask = mask.contiguous().view(-1, 1)   # [batch_size*seq_len-1, 1]\n",
        "        output = -input.gather(1, target) * mask # 在每个vocab_size维度取正确单词的索引， 但是里面有很多是填充进去的， 所以mask去掉这些填充的\n",
        "        # 这个其实在写一个NLloss ， 也就是sortmax的取负号\n",
        "        output = torch.sum(output) / torch.sum(mask)\n",
        "\n",
        "        return output  # [batch_size*seq_len-1, 1]"
      ],
      "metadata": {
        "id": "73vIqd2SmxJc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dropout = 0.2\n",
        "hidden_size = 100\n",
        "encoder = PlainEncoder(vocab_size=en_total_words, hidden_size=hidden_size, dropout=dropout)\n",
        "decoder = PlainDecoder(vocab_size=cn_total_words, hidden_size=hidden_size, dropout=dropout)\n",
        "\n",
        "model = PlainSeq2Seq(encoder, decoder)\n",
        "model = model.to(device)\n",
        "loss_fn = LanguageModelCriterion().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "crFALXxBm0HR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    total_num_words = total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
        "            mb_x = torch.from_numpy(mb_x).to(device).long()    # 这个是一个batch的英文句子 大小是[batch_size, seq_len]\n",
        "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()    # 每个句子的长度\n",
        "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()  # 解码器那边的输入， 输入一个单词去预测另外一个单词\n",
        "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()   # 解码器那边的输出  [batch_size, seq_len-1]\n",
        "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()  # 这个减去1， 因为没有了最后一个  [batch_size, seq_len-1]\n",
        "            mb_y_len[mb_y_len<=0] =  1   # 这句话是为了以防出错\n",
        "\n",
        "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
        "\n",
        "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
        "            # [batch_size, mb_y_len.max()], 上面是bool类型， 下面是float类型， 只计算每个句子的有效部分， 填充的那部分去掉\n",
        "            mb_out_mask = mb_out_mask.float()  # [batch_size, seq_len-1]  因为mb_y_len.max()就是seq_len-1\n",
        "\n",
        "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
        "\n",
        "            num_words = torch.sum(mb_y_len).item()\n",
        "            total_loss += loss.item() * num_words\n",
        "            total_num_words += num_words\n",
        "    print('Evaluation loss', total_loss / total_num_words)\n",
        "\n",
        "def train(model, data, num_epochs=20):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_num_words = total_loss = 0.\n",
        "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
        "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
        "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
        "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
        "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
        "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
        "            mb_y_len[mb_y_len<=0] = 1\n",
        "\n",
        "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
        "\n",
        "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
        "            mb_out_mask = mb_out_mask.float()\n",
        "\n",
        "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
        "\n",
        "            num_words = torch.sum(mb_y_len).item()\n",
        "            total_loss += loss.item() * num_words\n",
        "            total_num_words += num_words\n",
        "\n",
        "            # 更新\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)     # 这里防止梯度爆炸， 这是和以往不太一样的地方\n",
        "            optimizer.step()\n",
        "\n",
        "            if it % 100 == 0:\n",
        "                print('Epoch', epoch, 'iteration', it, 'loss', loss.item())\n",
        "\n",
        "        print('Epoch', epoch, 'Training loss', total_loss / total_num_words)\n",
        "        if epoch % 5 == 0:\n",
        "            evaluate(model, dev_data)\n",
        "\n",
        "# 训练"
      ],
      "metadata": {
        "id": "DiZDOMeFm43I"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_data, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xELzuPyTm60u",
        "outputId": "89d6aef7-bf40-41f8-834d-f7ccb6c05d37"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 loss 9.352622985839844\n",
            "Epoch 0 iteration 100 loss 5.8850297927856445\n",
            "Epoch 0 iteration 200 loss 5.447134494781494\n",
            "Epoch 0 Training loss 5.8665789826911405\n",
            "Evaluation loss 5.256462709692426\n",
            "Epoch 1 iteration 0 loss 5.378063678741455\n",
            "Epoch 1 iteration 100 loss 5.423961162567139\n",
            "Epoch 1 iteration 200 loss 5.042912483215332\n",
            "Epoch 1 Training loss 5.014470140454998\n",
            "Epoch 2 iteration 0 loss 5.04817008972168\n",
            "Epoch 2 iteration 100 loss 5.1299848556518555\n",
            "Epoch 2 iteration 200 loss 4.747646808624268\n",
            "Epoch 2 Training loss 4.682526725751429\n",
            "Epoch 3 iteration 0 loss 4.751145839691162\n",
            "Epoch 3 iteration 100 loss 4.848268032073975\n",
            "Epoch 3 iteration 200 loss 4.503965377807617\n",
            "Epoch 3 Training loss 4.4204412824598105\n",
            "Epoch 4 iteration 0 loss 4.499906063079834\n",
            "Epoch 4 iteration 100 loss 4.633659362792969\n",
            "Epoch 4 iteration 200 loss 4.347743034362793\n",
            "Epoch 4 Training loss 4.201119591361971\n",
            "Epoch 5 iteration 0 loss 4.286170482635498\n",
            "Epoch 5 iteration 100 loss 4.469820976257324\n",
            "Epoch 5 iteration 200 loss 4.17372989654541\n",
            "Epoch 5 Training loss 4.011637470945066\n",
            "Evaluation loss 4.506671529954519\n",
            "Epoch 6 iteration 0 loss 4.077154636383057\n",
            "Epoch 6 iteration 100 loss 4.274575710296631\n",
            "Epoch 6 iteration 200 loss 4.000185489654541\n",
            "Epoch 6 Training loss 3.8403287116099656\n",
            "Epoch 7 iteration 0 loss 3.9134469032287598\n",
            "Epoch 7 iteration 100 loss 4.135472774505615\n",
            "Epoch 7 iteration 200 loss 3.8659377098083496\n",
            "Epoch 7 Training loss 3.6856167860428872\n",
            "Epoch 8 iteration 0 loss 3.7778451442718506\n",
            "Epoch 8 iteration 100 loss 3.9892213344573975\n",
            "Epoch 8 iteration 200 loss 3.7165474891662598\n",
            "Epoch 8 Training loss 3.5403409392229936\n",
            "Epoch 9 iteration 0 loss 3.6424014568328857\n",
            "Epoch 9 iteration 100 loss 3.856553554534912\n",
            "Epoch 9 iteration 200 loss 3.5942752361297607\n",
            "Epoch 9 Training loss 3.4077155475592544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = torch.tensor([3, 8, 10, 2, 1])\n",
        "torch.arange(m.max().item())[None, :]   # tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsqOpwoSq_jR",
        "outputId": "4094d497-a071-4089-f0b1-8dba5350e256"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}